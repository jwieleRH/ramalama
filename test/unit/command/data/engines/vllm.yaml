schema_version: "1.0.0"
commands:
  - name: serve
    inference_engine:
      name: "vllm server"
      binary: "{{ '' if host.is_container else 'python3 -m vllm.entrypoints.openai.api_server' }}"
      options:
        - name: "--model"
          description: "The AI model to run"
          value: "{{ model.model_path }}"
        - name: "--served-model-name"
          description: "The name assigned to the run AI model"
          value: "{{ model.alias }}"
        - name: "--max_model_len"
          description: "Size of the model context"
          value: "{{ args.ctx_size }}"
          if: "{{ args.ctx_size > 0 }}"
        - name: "--port"
          description: "Port for the AI model server to listen on"
          value: "{{ args.port }}"
        - name: "--seed"
          description: "Seed the global PRNG"
          value: "{{ args.seed }}"
        - name: ""
          description: "Arbitrary runtime arguments for vLLM server"
          value: "{{ args.runtime_args }}"
